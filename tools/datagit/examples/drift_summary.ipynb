{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modification patterns:\n",
      "   unique_keys    column old_value         new_value           pattern_id\n",
      "0      [1, 17]  Quantity        30                35  8474870305201232022\n",
      "1          [1]     Sales       150               175  7566823910652056486\n",
      "2          [4]     Sales       500               550 -8991116805121846606\n",
      "3      [5, 13]   Product     Chair      Office Chair  2215092074679393991\n",
      "4          [5]     Sales       300               350 -7628166106107099279\n",
      "5          [7]  Quantity        15                12  8495371893006855378\n",
      "6          [7]     Sales       600               580  6050963463667707019\n",
      "7          [8]  Quantity        30                25  2196394033433131320\n",
      "8          [8]     Sales       300               280  8970732250856767725\n",
      "9          [9]  Quantity        50                60 -8704853741378330780\n",
      "10         [9]     Sales       250               300  3697450927660077567\n",
      "11    [10, 16]  Quantity        40                45  4449227044841137586\n",
      "12        [10]     Sales       200               225  8528413375589513385\n",
      "13        [11]   Product  Notebook           Journal  4419331654405882850\n",
      "14        [11]     Sales        90               100  8653519199984503954\n",
      "15        [13]  Quantity       100               110  6305122414284309607\n",
      "16        [13]     Sales       100               110  5960093754447864092\n",
      "17        [15]   Product    Marker  Permanent Marker  2546882750941807024\n",
      "18        [15]  Quantity        80                75 -9129223000334045526\n",
      "19        [15]     Sales       160               150 -4409240659754527617\n",
      "20        [16]     Sales       160               180 -6624555937265735662\n",
      "21        [17]   Product      Soap       Liquid Soap  8799611828381313806\n",
      "22        [17]     Sales        90               105 -4454808046348813247\n",
      "23        [19]  Quantity        70                90 -1256597059786698759\n",
      "24        [19]     Sales       140               180 -4887413111293880937\n",
      "25        [20]   Product       Mop           Swiffer -2415844805341036213\n",
      "26        [20]     Sales       125               130 -4597885732996989269\n",
      "\n",
      "Grouped patterns:\n",
      "pattern_id\n",
      "-9129223000334045526        [[15]]\n",
      "-8991116805121846606         [[4]]\n",
      "-8704853741378330780         [[9]]\n",
      "-7628166106107099279         [[5]]\n",
      "-6624555937265735662        [[16]]\n",
      "-4887413111293880937        [[19]]\n",
      "-4597885732996989269        [[20]]\n",
      "-4454808046348813247        [[17]]\n",
      "-4409240659754527617        [[15]]\n",
      "-2415844805341036213        [[20]]\n",
      "-1256597059786698759        [[19]]\n",
      " 2196394033433131320         [[8]]\n",
      " 2215092074679393991     [[5, 13]]\n",
      " 2546882750941807024        [[15]]\n",
      " 3697450927660077567         [[9]]\n",
      " 4419331654405882850        [[11]]\n",
      " 4449227044841137586    [[10, 16]]\n",
      " 5960093754447864092        [[13]]\n",
      " 6050963463667707019         [[7]]\n",
      " 6305122414284309607        [[13]]\n",
      " 7566823910652056486         [[1]]\n",
      " 8474870305201232022     [[1, 17]]\n",
      " 8495371893006855378         [[7]]\n",
      " 8528413375589513385        [[10]]\n",
      " 8653519199984503954        [[11]]\n",
      " 8799611828381313806        [[17]]\n",
      " 8970732250856767725         [[8]]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV files into DataFrames\n",
    "from datagit.drift_evaluators import DriftEvaluatorContext\n",
    "from datagit.dataframe_update_breakdown import summarize_dataframe_updates\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "initial_df = pd.read_csv('patterns/initial_df.csv')\n",
    "final_df = pd.read_csv('patterns/final_df.csv')\n",
    "\n",
    "# Use the function to summarize updates\n",
    "update_summary = summarize_dataframe_updates(DriftEvaluatorContext(before=initial_df, after=final_df))\n",
    "\n",
    "# # Display the results\n",
    "# print(\"Added rows:\")\n",
    "# print(update_summary['added'])\n",
    "\n",
    "# print(\"\\nDeleted rows:\")\n",
    "# print(update_summary['deleted'])\n",
    "\n",
    "print(\"\\nModification patterns:\")\n",
    "print(update_summary['modified_patterns'])\n",
    "\n",
    "grouped_df = update_summary['modified_patterns'].groupby('pattern_id').apply(lambda x: x['unique_keys'].tolist())\n",
    "print(\"\\nGrouped patterns:\")\n",
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    unique_key    Product     Category  Quantity  Sales\n",
      "0            1      Apple         Food        30    150\n",
      "1            2     Banana         Food        45    135\n",
      "2            3     Carrot         Food        50    100\n",
      "3            4       Desk    Furniture         5    500\n",
      "4            5      Chair    Furniture        10    300\n",
      "5            6       Lamp  Electronics        20    400\n",
      "6            7    Monitor  Electronics        15    600\n",
      "7            8   Keyboard  Electronics        30    300\n",
      "8            9      Mouse  Electronics        50    250\n",
      "9           10       Book   Stationery        40    200\n",
      "10          11   Notebook   Stationery        30     90\n",
      "11          12        Pen   Stationery        60    120\n",
      "12          13      Chair   Stationery       100    100\n",
      "13          14     Eraser   Stationery       150     75\n",
      "14          15     Marker   Stationery        80    160\n",
      "15          16  Detergent     Cleaning        40    160\n",
      "16          17       Soap     Cleaning        30     90\n",
      "17          18     Bleach     Cleaning        50    250\n",
      "18          19     Sponge     Cleaning        70    140\n",
      "19          20        Mop     Cleaning        25    125\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/sammyteillet/Documents/Projects/DataDrift/data-drift/tools/datagit/examples/drift_summary.ipynb Cell 2\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sammyteillet/Documents/Projects/DataDrift/data-drift/tools/datagit/examples/drift_summary.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m final_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mpatterns/final_df.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sammyteillet/Documents/Projects/DataDrift/data-drift/tools/datagit/examples/drift_summary.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(initial_df)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sammyteillet/Documents/Projects/DataDrift/data-drift/tools/datagit/examples/drift_summary.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m result \u001b[39m=\u001b[39m dataframe_update_breakdown(initial_df, final_df)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sammyteillet/Documents/Projects/DataDrift/data-drift/tools/datagit/examples/drift_summary.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/Documents/Projects/DataDrift/data-drift/tools/datagit/datagit/dataframe_update_breakdown.py:37\u001b[0m, in \u001b[0;36mdataframe_update_breakdown\u001b[0;34m(initial_dataframe, final_dataframe)\u001b[0m\n\u001b[1;32m     31\u001b[0m step1 \u001b[39m=\u001b[39m initial_dataframe\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(columns_removed))\n\u001b[1;32m     33\u001b[0m \u001b[39m# TODO handle case when there is not\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# new_data = initial_dataframe\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# if \"date\" in initial_dataframe.columns and \"date\" in final_dataframe.columns:\u001b[39;00m\n\u001b[1;32m     36\u001b[0m new_data \u001b[39m=\u001b[39m final_dataframe\u001b[39m.\u001b[39mloc[\n\u001b[0;32m---> 37\u001b[0m     \u001b[39m~\u001b[39mfinal_dataframe[\u001b[39m\"\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39misin(initial_dataframe[\u001b[39m\"\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     38\u001b[0m ]\n\u001b[1;32m     40\u001b[0m step2 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([step1, new_data[step1\u001b[39m.\u001b[39mcolumns]], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     42\u001b[0m step3 \u001b[39m=\u001b[39m final_dataframe\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(columns_added))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "from datagit.dataframe_update_breakdown import dataframe_update_breakdown\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "initial_df = pd.read_csv('patterns/initial_df.csv')\n",
    "final_df = pd.read_csv('patterns/final_df.csv')\n",
    "print(initial_df)\n",
    "\n",
    "result = dataframe_update_breakdown(initial_df, final_df)\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
