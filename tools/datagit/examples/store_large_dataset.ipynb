{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from github import Github\n",
    "import os\n",
    "\n",
    "# Create a .env with the following content:\n",
    "# GH_TOKEN=your_github_token\n",
    "# REPON=$gh_org/$repo\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "\n",
    "# Get GitHub token from environment variable\n",
    "gh_token = os.getenv(\"GH_TOKEN\")\n",
    "if gh_token is None:\n",
    "    print(\"GitHub token not found! Create a .env file a the root with a GH_TOKEN variable.\")\n",
    "    exit(1)\n",
    "gh_client = Github(gh_token, timeout=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  unique_key        date  metric_value  \\\n",
      "0       2d7c63e0-6064-4045-a2e4-f24f54e2ee39  2023-03-25          2.35   \n",
      "1       d80ee206-0e5b-4f33-a464-fe4479bb1472  2023-06-20          8.37   \n",
      "2       1bbabd1c-b512-4878-80ed-4cdbdc714b73  2022-12-04          6.39   \n",
      "3       34c55387-769d-4657-8431-191f988993a7  2023-06-21          3.08   \n",
      "4       6882e07d-66d5-4a91-9c4e-969c3a69391c  2023-04-25          9.81   \n",
      "...                                      ...         ...           ...   \n",
      "599995  ee0ac304-4e16-4102-b6a7-bb53a4ff6295  2022-10-07          4.28   \n",
      "599996  e2054c3a-4aad-446d-a588-16d6dc084436  2022-11-25          4.92   \n",
      "599997  af928223-d9f6-4d3e-b1b8-8347eb140169  2023-01-24          5.52   \n",
      "599998  69b148b8-3ea9-4cee-961c-03b2f0c0f8ef  2023-04-02          4.13   \n",
      "599999  f3d3279b-be6f-4fa3-8b38-2278f109f799  2022-10-29          2.99   \n",
      "\n",
      "       country_code    category  \n",
      "0                KZ  Category C  \n",
      "1                FJ  Category A  \n",
      "2                LT  Category B  \n",
      "3                UA  Category C  \n",
      "4                LV  Category B  \n",
      "...             ...         ...  \n",
      "599995           ES  Category A  \n",
      "599996           DM  Category B  \n",
      "599997           SL  Category B  \n",
      "599998           UA  Category B  \n",
      "599999           EC  Category A  \n",
      "\n",
      "[600000 rows x 5 columns]\n",
      "Size of DataFrame in bytes: 40140520\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker to generate random data\n",
    "fake = Faker()\n",
    "\n",
    "# Set the number of rows for the dataframe\n",
    "num_rows = 600000\n",
    "\n",
    "# Generate random IDs and dates\n",
    "ids = [fake.uuid4() for _ in range(num_rows)]\n",
    "dates = [fake.date_between(start_date='-1y', end_date='-1m').strftime('%Y-%m-%d') for _ in range(num_rows)]\n",
    "\n",
    "# Generate random metric values between 0 and 10\n",
    "metric_values = [round(random.uniform(0, 10),2) for _ in range(num_rows)]\n",
    "# Generate random country codes\n",
    "country_codes = [fake.country_code() for _ in range(num_rows)]\n",
    "\n",
    "# Generate random categories\n",
    "categories = [random.choice(['Category A', 'Category B', 'Category C']) for _ in range(num_rows)]\n",
    "\n",
    "# Create the dataframe\n",
    "ultra_large_df = pd.DataFrame({'unique_key': ids, 'date': dates, 'metric_value': metric_values, 'country_code': country_codes, 'category': categories})\n",
    "\n",
    "# Print the dataframe\n",
    "print(ultra_large_df)\n",
    "local_file_path = 'ultra_large_df.csv'\n",
    "ultra_large_df.to_csv(local_file_path, index=False)\n",
    "print('Size of DataFrame in bytes:', os.path.getsize(local_file_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing metric...\n",
      "Metric not found, creating it on branch: main\n",
      "Commit: New data: path/to/ultra_large_metric_name18.csv\n",
      "Metric stored\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import importlib\n",
    "import datagit.github_connector\n",
    "importlib.reload(datagit.github_connector)\n",
    "from datagit.github_connector import store_metric\n",
    "\n",
    "## Test with file already existing and splitting new data and historical data\n",
    "repo = os.getenv(\"REPO\") or \"gh_org/repo\"\n",
    "file_path = repo+\"/path/to/ultra_large_metric_name18.csv\"\n",
    "store_metric(gh_client,  ultra_large_df, file_path, assignees=[\"Sammy\"], store_json=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4242  60608 392832  41643 464234 122681  10258 199077 303125 520722\n",
      " 589338 285923 340236 339287 170118 520994 591772 550478 455232 123979\n",
      " 354761 254889 470580 181624 274417 328726 504212 356040  23352 214898\n",
      " 361248 277103 285786 356048  23550 236107 256942 509629 225513 564335\n",
      " 184036 354122 408248 516338 367441 413740 222624 558355 350732  39333\n",
      " 281914 445136 158145 571695  68129 364739 451960 551108 127251 528991\n",
      " 179870 101684 296154 579076 578190 404752  22682 336015 431689 505472\n",
      " 470965 267651 454329 439546 337991 326490 521320 586039 460422 513493\n",
      "  29421 382580  56964  40928  17648 511244  31867 208736 367384 157223\n",
      " 503369 483668  45568 248656 413335  14685 460849  72643 396418 483375]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "ultra_large_df2 = ultra_large_df.copy()\n",
    "\n",
    "# Select 10 random indices for metric value update\n",
    "random_indices_metric = np.random.choice(ultra_large_df2.index, size=100, replace=False)\n",
    "print(random_indices_metric)\n",
    "# Update metric value with random values between 0 and 10\n",
    "ultra_large_df2.loc[random_indices_metric, 'metric_value'] = [round(random.uniform(0, 10),2) for _ in range(100)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unique_key       object\n",
       "date             object\n",
       "metric_value    float64\n",
       "country_code     object\n",
       "category         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ultra_large_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing metric...\n",
      "Metric found, updating it on branch: reported\n",
      "Content https://raw.githubusercontent.com/Samox/data-history/reported/path/to/ultra_large_metric_name14.csv?token=ABUWFP6JIN2WYROOIDT4PDDEW2BPQ\n",
      "Dataframe dtypes {'unique_key': string[python], 'date': string[python], 'metric_value': string[python], 'country_code': string[python], 'category': string[python]}\n",
      "Old Dataframe dtypes {'unique_key': string[python], 'date': string[python], 'metric_value': string[python], 'country_code': string[python], 'category': string[python]}\n",
      "comparison                                      metric_value      \n",
      "                                             self other\n",
      "unique_key                                             \n",
      "039d29a6-a84e-495f-89ab-9a64cc2f283a         6.37  1.89\n",
      "0af6ef89-87c5-498f-b37c-6629d273f87c         6.45  0.73\n",
      "0b6cc7f7-03c9-4e91-bc22-ba915a448529         0.95  9.86\n",
      "0de03d6e-05ed-41b1-8b8f-e0ae64c0cbd8         3.45  2.92\n",
      "0e1a763a-7498-4934-b836-ea4d6f08efc1         8.91  6.97\n",
      "...                                           ...   ...\n",
      "ec10e0bc-ea52-40df-a04c-480b86f6cfdf         2.65  5.94\n",
      "efafef2c-6a21-4936-a15d-4ba7fecf1343         6.83  0.71\n",
      "f29b5104-d3ef-4424-ae58-d07b74e64689         7.26  4.35\n",
      "f8fcf6e0-23ed-44ce-bc51-d7d23959d2c9          0.8  2.69\n",
      "f9f7b2b8-a88b-42c4-b33c-2ec3ee11e3e5         4.26  2.05\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "Drift detected\n",
      "Drift evaluation: {'should_alert': True, 'message': 'Drift detected:\\n- ~~🆕 0 addition~~\\n- ♻️ 100 modifications\\n- ~~🗑️ 0 deletion~~'}\n",
      "Commit: Drift: path/to/ultra_large_metric_name14.csv\n",
      "https://github.com/Samox/data-history/commit/56f78c81de67a58cfe56e4ba2b7c85ac7907a774\n"
     ]
    },
    {
     "ename": "GithubException",
     "evalue": "502 {\"message\": \"Server Error\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGithubException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m importlib\u001b[39m.\u001b[39mreload(datagit\u001b[39m.\u001b[39mgithub_connector)\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatagit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgithub_connector\u001b[39;00m \u001b[39mimport\u001b[39;00m store_metric\n\u001b[0;32m---> 15\u001b[0m store_metric(gh_client,  ultra_large_df2, file_path, assignees\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mSammy\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/Documents/Projects/DataDrift/data-drift/tools/datagit/examples/../datagit/github_connector.py:46\u001b[0m, in \u001b[0;36mstore_metric\u001b[0;34m(ghClient, dataframe, filepath, assignees, branch, store_json, drift_evaluator)\u001b[0m\n\u001b[1;32m     43\u001b[0m repo \u001b[39m=\u001b[39m ghClient\u001b[39m.\u001b[39mget_repo(repo_orga \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m repo_name)\n\u001b[1;32m     44\u001b[0m dataframe \u001b[39m=\u001b[39m sort_dataframe_on_first_column_and_assert_is_unique(dataframe)\n\u001b[0;32m---> 46\u001b[0m push_metric(\n\u001b[1;32m     47\u001b[0m     dataframe,\n\u001b[1;32m     48\u001b[0m     assignees,\n\u001b[1;32m     49\u001b[0m     repo\u001b[39m.\u001b[39;49mdefault_branch,\n\u001b[1;32m     50\u001b[0m     branch,\n\u001b[1;32m     51\u001b[0m     store_json,\n\u001b[1;32m     52\u001b[0m     file_path,\n\u001b[1;32m     53\u001b[0m     repo,\n\u001b[1;32m     54\u001b[0m     drift_evaluator,\n\u001b[1;32m     55\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Projects/DataDrift/data-drift/tools/datagit/examples/../datagit/github_connector.py:147\u001b[0m, in \u001b[0;36mpush_metric\u001b[0;34m(dataframe, assignees, reported_branch, computed_branch, store_json, file_path, repo, drift_evaluator)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDrift evaluation: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(drift_evaluation))\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m drift_evaluation[\u001b[39m\"\u001b[39m\u001b[39mshould_alert\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 147\u001b[0m     push_drift_lines(\n\u001b[1;32m    148\u001b[0m         file_path, repo, computed_branch, dataframe, store_json\n\u001b[1;32m    149\u001b[0m     )\n\u001b[1;32m    150\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDrift pushed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCreating pull request\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Projects/DataDrift/data-drift/tools/datagit/examples/../datagit/github_connector.py:240\u001b[0m, in \u001b[0;36mpush_drift_lines\u001b[0;34m(file_path, repo, branch, dataframe, store_json, commit_body)\u001b[0m\n\u001b[1;32m    238\u001b[0m json_commit_message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDrift (json): \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m json_file_path\n\u001b[1;32m    239\u001b[0m json_data \u001b[39m=\u001b[39m dataframe\u001b[39m.\u001b[39mto_json(orient\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrecords\u001b[39m\u001b[39m\"\u001b[39m, lines\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, date_format\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39miso\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 240\u001b[0m update_file_with_retry(\n\u001b[1;32m    241\u001b[0m     repo, json_file_path, json_commit_message, json_data, branch\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Projects/DataDrift/data-drift/tools/datagit/examples/../datagit/github_connector.py:267\u001b[0m, in \u001b[0;36mupdate_file_with_retry\u001b[0;34m(repo, file_path, commit_message, data, branch, max_retries)\u001b[0m\n\u001b[1;32m    265\u001b[0m             time\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)  \u001b[39m# Wait for 1 second before retrying\u001b[39;00m\n\u001b[1;32m    266\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    268\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to update file after \u001b[39m\u001b[39m{\u001b[39;00mmax_retries\u001b[39m}\u001b[39;00m\u001b[39m retries\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Projects/DataDrift/data-drift/tools/datagit/examples/../datagit/github_connector.py:257\u001b[0m, in \u001b[0;36mupdate_file_with_retry\u001b[0;34m(repo, file_path, commit_message, data, branch, max_retries)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[39mprint\u001b[39m(response[\u001b[39m\"\u001b[39m\u001b[39mcommit\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mhtml_url)\n\u001b[1;32m    256\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 257\u001b[0m     response \u001b[39m=\u001b[39m repo\u001b[39m.\u001b[39;49mupdate_file(\n\u001b[1;32m    258\u001b[0m         file_path, commit_message, data, content\u001b[39m.\u001b[39;49msha, branch\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    260\u001b[0m     \u001b[39mprint\u001b[39m(response[\u001b[39m\"\u001b[39m\u001b[39mcommit\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mhtml_url)\n\u001b[1;32m    261\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/github/Repository.py:2154\u001b[0m, in \u001b[0;36mRepository.update_file\u001b[0;34m(self, path, message, content, sha, branch, committer, author)\u001b[0m\n\u001b[1;32m   2151\u001b[0m \u001b[39mif\u001b[39;00m committer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m github\u001b[39m.\u001b[39mGithubObject\u001b[39m.\u001b[39mNotSet:\n\u001b[1;32m   2152\u001b[0m     put_parameters[\u001b[39m\"\u001b[39m\u001b[39mcommitter\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m committer\u001b[39m.\u001b[39m_identity\n\u001b[0;32m-> 2154\u001b[0m headers, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_requester\u001b[39m.\u001b[39;49mrequestJsonAndCheck(\n\u001b[1;32m   2155\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mPUT\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   2156\u001b[0m     \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murl\u001b[39m}\u001b[39;49;00m\u001b[39m/contents/\u001b[39;49m\u001b[39m{\u001b[39;49;00murllib\u001b[39m.\u001b[39;49mparse\u001b[39m.\u001b[39;49mquote(path)\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   2157\u001b[0m     \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mput_parameters,\n\u001b[1;32m   2158\u001b[0m )\n\u001b[1;32m   2160\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m   2161\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcommit\u001b[39m\u001b[39m\"\u001b[39m: github\u001b[39m.\u001b[39mCommit\u001b[39m.\u001b[39mCommit(\n\u001b[1;32m   2162\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_requester, headers, data[\u001b[39m\"\u001b[39m\u001b[39mcommit\u001b[39m\u001b[39m\"\u001b[39m], completed\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2166\u001b[0m     ),\n\u001b[1;32m   2167\u001b[0m }\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/github/Requester.py:353\u001b[0m, in \u001b[0;36mRequester.requestJsonAndCheck\u001b[0;34m(self, verb, url, parameters, headers, input)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequestJsonAndCheck\u001b[39m(\u001b[39mself\u001b[39m, verb, url, parameters\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, headers\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__check(\n\u001b[1;32m    354\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequestJson(\n\u001b[1;32m    355\u001b[0m             verb, url, parameters, headers, \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__customConnection(url)\n\u001b[1;32m    356\u001b[0m         )\n\u001b[1;32m    357\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/github/Requester.py:378\u001b[0m, in \u001b[0;36mRequester.__check\u001b[0;34m(self, status, responseHeaders, output)\u001b[0m\n\u001b[1;32m    376\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__structuredFromJson(output)\n\u001b[1;32m    377\u001b[0m \u001b[39mif\u001b[39;00m status \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m400\u001b[39m:\n\u001b[0;32m--> 378\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__createException(status, responseHeaders, output)\n\u001b[1;32m    379\u001b[0m \u001b[39mreturn\u001b[39;00m responseHeaders, output\n",
      "\u001b[0;31mGithubException\u001b[0m: 502 {\"message\": \"Server Error\"}"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import importlib\n",
    "\n",
    "import datagit.drift_evaluators\n",
    "importlib.reload(datagit.drift_evaluators)\n",
    "from datagit.drift_evaluators import default_drift_evaluator\n",
    "\n",
    "\n",
    "import datagit.github_connector\n",
    "importlib.reload(datagit.github_connector)\n",
    "from datagit.github_connector import store_metric\n",
    "\n",
    "\n",
    "store_metric(gh_client,  ultra_large_df2, file_path, assignees=[\"Sammy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of rows for the dataframe\n",
    "num_rows = 6000\n",
    "\n",
    "# Generate random IDs and dates\n",
    "ids = [fake.uuid4() for _ in range(num_rows)]\n",
    "dates = [fake.date_between(start_date='-1m', end_date='today').strftime('%Y-%m-%d') for _ in range(num_rows)]\n",
    "\n",
    "# Generate random metric values between 0 and 10\n",
    "metric_values = [round(random.uniform(0, 10),2) for _ in range(num_rows)]\n",
    "# Generate random country codes\n",
    "country_codes = [fake.country_code() for _ in range(num_rows)]\n",
    "\n",
    "# Generate random categories\n",
    "categories = [random.choice(['Category A', 'Category B', 'Category C']) for _ in range(num_rows)]\n",
    "\n",
    "# Create the dataframe\n",
    "new_lines = pd.DataFrame({'unique_key': ids, 'date': dates, 'metric_value': metric_values, 'country_code': country_codes, 'category': categories})\n",
    "\n",
    "ultra_large_df3 = pd.concat([ultra_large_df2, new_lines], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing metric...\n",
      "Metric found, updating it on branch: main\n",
      "Content https://raw.githubusercontent.com/Samox/data-history/main/path/to/ultra_large_metric_name18.csv?token=ABUWFP5KQIF72QPM6CKZ6VTEW2IZK\n",
      "Dataframe dtypes {'unique_key': string[python], 'date': string[python], 'metric_value': string[python], 'country_code': string[python], 'category': string[python]}\n",
      "Old Dataframe dtypes {'unique_key': string[python], 'date': string[python], 'metric_value': string[python], 'country_code': string[python], 'category': string[python]}\n",
      "New data found\n",
      "Commit: New data: path/to/ultra_large_metric_name18.csv\n",
      "https://github.com/Samox/data-history/commit/510cc1a3978ec68cc439c5f19088d9626eb0db2f\n",
      "Branch metric/path-to-ultra-large-metric-name18-csv doesn't exist, creating it...\n",
      "comparison                                      metric_value      \n",
      "                                             self other\n",
      "unique_key                                             \n",
      "03aeb072-4c9b-4900-a65f-0cb012459f50         5.28  2.02\n",
      "0500de9f-7c2c-4c7a-8f6e-7d2be08551a0         7.35  4.17\n",
      "0b1e939d-15d8-446f-82e0-0774a42b6e38         8.08  1.18\n",
      "0dbaf1fc-88be-4641-8d79-914939080efe         5.51  3.34\n",
      "0fe39883-35c3-45c9-bcff-e84caf85ff39         4.96  0.21\n",
      "...                                           ...   ...\n",
      "ea4f01d7-1c35-406f-830e-f72218d76617         2.99   6.7\n",
      "efa5f736-759e-477a-bc60-ee39a6acf360         5.99  1.34\n",
      "f096b302-e842-48b0-8d73-9eac7917c5fb         7.66  6.84\n",
      "f7a7d70b-f652-4503-acd6-df3fe7e1ab70          9.6  1.51\n",
      "fcdcf0f6-eeb6-43f7-82d7-512c27926639         5.15  8.64\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "Drift detected\n",
      "Drift evaluation: {'should_alert': True, 'message': 'Drift detected:\\n- ~~🆕 0 addition~~\\n- ♻️ 605988 modifications\\n- ~~🗑️ 0 deletion~~'}\n",
      "Commit: Drift: path/to/ultra_large_metric_name18.csv\n",
      "https://github.com/Samox/data-history/commit/2acacfc24f637533b7a5f987a7d268d3c792a54b\n",
      "Drift pushed\n",
      "Creating pull request\n",
      "Pull request created: https://github.com/Samox/data-history/pull/155\n",
      "Assignee Sammy does not exist\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import importlib\n",
    "\n",
    "import datagit.drift_evaluators\n",
    "importlib.reload(datagit.drift_evaluators)\n",
    "from datagit.drift_evaluators import default_drift_evaluator\n",
    "\n",
    "\n",
    "import datagit.github_connector\n",
    "importlib.reload(datagit.github_connector)\n",
    "from datagit.github_connector import store_metric\n",
    "\n",
    "store_metric(gh_client,  ultra_large_df3, file_path, assignees=[\"Sammy\"], store_json=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultra_large_df.to_json(local_file_path+'.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
